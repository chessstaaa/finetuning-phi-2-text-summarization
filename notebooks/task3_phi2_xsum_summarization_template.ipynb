{"cells":[{"cell_type":"markdown","id":"b122fb72","metadata":{"id":"b122fb72"},"source":["# Task 3 â€” Fine-tuning Decoder-only LLM (Phi-2) untuk Summarization (XSum)\n","\n","**Target UAS:** fine-tune model decoder-only (Phi-2) untuk membuat ringkasan abstraktif pada dataset XSum.\n","\n","**Catatan resource:** Phi-2 relatif besar. Banyak orang memakai PEFT/LoRA + 4-bit quantization agar muat di GPU terbatas.\n","- Template ini menyediakan jalur **LoRA + 4-bit** (opsional).\n","- Jika kamu full fine-tune, kamu mungkin butuh GPU memory besar.\n","\n","Tanggal template: 2026-01-05"]},{"cell_type":"markdown","id":"fa36cb8d","metadata":{"id":"fa36cb8d"},"source":["## 0. Setup\n","**TODO:** pastikan `bitsandbytes` kompatibel dengan environment kamu (terutama di Windows/local).\n","\n","Jika kamu tidak bisa memakai 4-bit, kamu bisa:\n","- pakai LoRA tanpa quantization (butuh VRAM lebih)\n","- atau pakai model lebih kecil (jika diizinkan)"]},{"cell_type":"code","execution_count":null,"id":"093e0d82","metadata":{"id":"093e0d82"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import torch\n","\n","from datasets import load_dataset\n","import evaluate\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorForLanguageModeling,\n","    set_seed,\n","    BitsAndBytesConfig,\n",")\n","\n","# Optional (PEFT)\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","\n","SEED = 42\n","set_seed(SEED)\n","np.random.seed(SEED)\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"device:\", device)"]},{"cell_type":"markdown","id":"0804cf04","metadata":{"id":"0804cf04"},"source":["## 1. Load dataset & model\n","\n","**TODO:** pastikan nama model Phi-2 benar sesuai HuggingFace Hub yang kamu pakai.\n","\n","Dataset XSum fields umumnya:\n","- `document`\n","- `summary`"]},{"cell_type":"code","execution_count":null,"id":"a0bf2f8e","metadata":{"id":"a0bf2f8e"},"outputs":[],"source":["DATASET_NAME = \"xsum\"\n","\n","# TODO: pastikan nama model sesuai yang tersedia di HuggingFace Hub\n","MODEL_NAME = \"microsoft/phi-2\"\n","\n","MAX_LENGTH = 512         # panjang prompt+target setelah tokenisasi\n","MAX_DOC_CHARS = 4000     # pembatas karakter agar dokumen tidak terlalu panjang (TODO: sesuaikan)\n","\n","LR = 2e-4\n","BATCH_SIZE = 2\n","EPOCHS = 1\n","GRAD_ACCUM = 8\n","\n","USE_4BIT_LORA = True  # TODO: ubah jika ingin full fine-tune / LoRA tanpa 4-bit"]},{"cell_type":"code","execution_count":null,"id":"56f33ac7","metadata":{"id":"56f33ac7"},"outputs":[],"source":["dataset = load_dataset(DATASET_NAME)\n","print(dataset)\n","metric = evaluate.load(\"rouge\")"]},{"cell_type":"markdown","id":"bc5a1506","metadata":{"id":"bc5a1506"},"source":["## 2. Load tokenizer & model\n","\n","Untuk model causal LM:\n","- kita membuat prompt \"Summarize: {document}\\nSummary:\" lalu targetnya `summary`\n","- saat training, label biasanya sama dengan input_ids (shift internal oleh model)\n","\n","**TODO:** cek `tokenizer.pad_token` (beberapa LLM tidak punya pad token)."]},{"cell_type":"code","execution_count":null,"id":"99e7ddf4","metadata":{"id":"99e7ddf4"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n","\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","if USE_4BIT_LORA:\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.float16,\n","        bnb_4bit_use_double_quant=True,\n","    )\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_NAME,\n","        quantization_config=bnb_config,\n","        device_map=\"auto\",\n","    )\n","    model = prepare_model_for_kbit_training(model)\n","\n","    lora_config = LoraConfig(\n","        r=16,\n","        lora_alpha=32,\n","        lora_dropout=0.05,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","    )\n","    model = get_peft_model(model, lora_config)\n","    model.print_trainable_parameters()\n","else:\n","    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)"]},{"cell_type":"markdown","id":"9e396369","metadata":{"id":"9e396369"},"source":["## 3. Preprocessing\n","\n","**TODO:** jika butuh cepat, bisa subset dataset (misalnya 10k contoh)."]},{"cell_type":"code","execution_count":null,"id":"08aa2936","metadata":{"id":"08aa2936"},"outputs":[],"source":["def build_prompt(doc: str) -> str:\n","    doc = doc[:MAX_DOC_CHARS]\n","    return f\"Summarize the following article in 1-2 sentences.\\n\\nArticle:\\n{doc}\\n\\nSummary:\"\n","\n","def preprocess_batch(examples):\n","    docs = examples[\"document\"]\n","    sums = examples[\"summary\"]\n","\n","    texts = []\n","    for d, s in zip(docs, sums):\n","        prompt = build_prompt(d)\n","        # Untuk causal LM training sederhana: gabungkan prompt + target + EOS\n","        full = prompt + \" \" + s + tokenizer.eos_token\n","        texts.append(full)\n","\n","    tokenized = tokenizer(\n","        texts,\n","        truncation=True,\n","        max_length=MAX_LENGTH,\n","        padding=False,\n","    )\n","    # labels = input_ids (standard causal LM)\n","    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n","    return tokenized\n","\n","tokenized = dataset.map(preprocess_batch, batched=True, remove_columns=dataset[\"train\"].column_names)\n","print(tokenized)"]},{"cell_type":"markdown","id":"5cdb6305","metadata":{"id":"5cdb6305"},"source":["## 4. Trainer\n","\n","**TODO:** atur output_dir & logging."]},{"cell_type":"code","execution_count":null,"id":"2e8db095","metadata":{"id":"2e8db095"},"outputs":[],"source":["data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","args = TrainingArguments(\n","    output_dir=\"outputs\",\n","    learning_rate=LR,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    gradient_accumulation_steps=GRAD_ACCUM,\n","    num_train_epochs=EPOCHS,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=500,\n","    save_steps=500,\n","    logging_steps=50,\n","    fp16=torch.cuda.is_available(),\n","    report_to=\"none\",\n","    save_total_limit=2,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=tokenized[\"train\"],\n","    eval_dataset=tokenized[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"markdown","id":"baa80a3f","metadata":{"id":"baa80a3f"},"source":["## 5. Training\n","\n","**TODO:** jalankan training."]},{"cell_type":"code","execution_count":null,"id":"407a3706","metadata":{"id":"407a3706"},"outputs":[],"source":["# trainer.train()"]},{"cell_type":"markdown","id":"46ac57b5","metadata":{"id":"46ac57b5"},"source":["## 6. Evaluasi ROUGE (setelah training)\n","\n","Evaluasi summarization biasanya:\n","- generate summary dari prompt\n","- bandingkan dengan reference summary (ROUGE)\n","\n","**TODO:** jalankan cell ini setelah training (dan mungkin pakai subset agar cepat)."]},{"cell_type":"code","execution_count":null,"id":"fb0f1015","metadata":{"id":"fb0f1015"},"outputs":[],"source":["# def generate_summary(doc: str, max_new_tokens: int = 64):\n","#     prompt = build_prompt(doc)\n","#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","#     with torch.no_grad():\n","#         out = model.generate(\n","#             **inputs,\n","#             max_new_tokens=max_new_tokens,\n","#             do_sample=False,\n","#             num_beams=4,\n","#         )\n","#     text = tokenizer.decode(out[0], skip_special_tokens=True)\n","#     # Ambil teks setelah \"Summary:\"\n","#     if \"Summary:\" in text:\n","#         text = text.split(\"Summary:\", 1)[-1].strip()\n","#     return text\n","\n","# # Quick eval on small subset\n","# n = 200\n","# preds, refs = [], []\n","# for ex in dataset[\"validation\"].select(range(n)):\n","#     preds.append(generate_summary(ex[\"document\"]))\n","#     refs.append(ex[\"summary\"])\n","\n","# rouge = metric.compute(predictions=preds, references=refs)\n","# print(rouge)"]},{"cell_type":"markdown","id":"070ffde6","metadata":{"id":"070ffde6"},"source":["## 7. Analisis\n","\n","**TODO:** isi `reports/` dengan:\n","- ROUGE score\n","- contoh hasil summary bagus vs buruk\n","- diskusi abstractive vs extractive\n","- kendala truncation & panjang dokumen"]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}