{"cells":[{"cell_type":"markdown","id":"b122fb72","metadata":{"id":"b122fb72"},"source":["# Task 3 — Fine-tuning Decoder-only LLM (Phi-2) untuk Summarization (XSum)\n","\n","**Target UAS:** fine-tune model decoder-only (Phi-2) untuk membuat ringkasan abstraktif pada dataset XSum.\n","\n","**Catatan resource:** Phi-2 relatif besar. Banyak orang memakai PEFT/LoRA + 4-bit quantization agar muat di GPU terbatas.\n","- Template ini menyediakan jalur **LoRA + 4-bit** (opsional).\n","- Jika kamu full fine-tune, kamu mungkin butuh GPU memory besar.\n","\n","Tanggal template: 2026-01-05"]},{"cell_type":"markdown","id":"fa36cb8d","metadata":{"id":"fa36cb8d"},"source":["## 0. Setup\n","**TODO:** pastikan `bitsandbytes` kompatibel dengan environment kamu (terutama di Windows/local).\n","\n","Jika kamu tidak bisa memakai 4-bit, kamu bisa:\n","- pakai LoRA tanpa quantization (butuh VRAM lebih)\n","- atau pakai model lebih kecil (jika diizinkan)"]},{"cell_type":"code","execution_count":1,"id":"C-IjujKVGBA2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C-IjujKVGBA2","outputId":"88bfb8e3-0eb8-44c1-b87e-4b980d48fd62","executionInfo":{"status":"ok","timestamp":1768120383831,"user_tz":-420,"elapsed":28427,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","PROJECT_DIR: /content/drive/MyDrive/finetuning-phi-2-text-summarization\n","OUTPUTS_DIR: /content/drive/MyDrive/finetuning-phi-2-text-summarization/outputs\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","import os\n","from pathlib import Path\n","\n","PROJECT_DIR = \"/content/drive/MyDrive/finetuning-phi-2-text-summarization\"\n","PROJECT_DIR = Path(PROJECT_DIR)\n","\n","OUTPUTS_DIR = PROJECT_DIR / \"outputs\"\n","REPORTS_DIR = PROJECT_DIR / \"reports\"\n","MODELS_DIR  = PROJECT_DIR / \"models\"\n","\n","OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n","REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n","MODELS_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# (opsional) simpan cache HF di Drive biar nggak download ulang\n","os.environ[\"HF_HOME\"] = str(PROJECT_DIR / \".hf_home\")\n","os.environ[\"HF_DATASETS_CACHE\"] = str(PROJECT_DIR / \".hf_datasets_cache\")\n","os.environ[\"TRANSFORMERS_CACHE\"] = str(PROJECT_DIR / \".hf_transformers_cache\")\n","\n","print(\"PROJECT_DIR:\", PROJECT_DIR)\n","print(\"OUTPUTS_DIR:\", OUTPUTS_DIR)\n"]},{"cell_type":"code","execution_count":2,"id":"H4t6DgiCGn7W","metadata":{"id":"H4t6DgiCGn7W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768120395470,"user_tz":-420,"elapsed":11634,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"06c7998f-0e39-4bbd-d568-4912494d1196"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip -q install -U datasets evaluate transformers accelerate peft rouge-score bitsandbytes huggingface_hub\n"]},{"cell_type":"code","execution_count":3,"id":"093e0d82","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"093e0d82","outputId":"09de6b3b-fd7a-4a9c-f2f9-552499c2384d","executionInfo":{"status":"ok","timestamp":1768120418204,"user_tz":-420,"elapsed":22732,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["device: cuda\n","BNB_CONFIG_AVAILABLE: True\n","PEFT_AVAILABLE: True\n"]}],"source":["import os\n","import random\n","import numpy as np\n","import torch\n","\n","from datasets import load_dataset\n","import evaluate\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorForLanguageModeling,\n","    set_seed,\n",")\n","\n","# Optional: BitsAndBytes (4-bit quantization). Availability depends on transformers + bitsandbytes.\n","try:\n","    from transformers import BitsAndBytesConfig\n","    BNB_CONFIG_AVAILABLE = True\n","except Exception:\n","    BitsAndBytesConfig = None\n","    BNB_CONFIG_AVAILABLE = False\n","\n","# Optional: PEFT/LoRA\n","try:\n","    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","    PEFT_AVAILABLE = True\n","except Exception:\n","    LoraConfig = None\n","    get_peft_model = None\n","    prepare_model_for_kbit_training = None\n","    PEFT_AVAILABLE = False\n","\n","SEED = 42\n","set_seed(SEED)\n","np.random.seed(SEED)\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"device:\", device)\n","print(\"BNB_CONFIG_AVAILABLE:\", BNB_CONFIG_AVAILABLE)\n","print(\"PEFT_AVAILABLE:\", PEFT_AVAILABLE)\n"]},{"cell_type":"code","execution_count":4,"id":"HeimG6GvGwB1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HeimG6GvGwB1","outputId":"ce6f4d89-0cc9-429b-bb49-9981218df6a2","executionInfo":{"status":"ok","timestamp":1768120418211,"user_tz":-420,"elapsed":5,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["USE_4BIT: True\n","USE_LORA: True\n"]}],"source":["USE_4BIT = (device == \"cuda\") and BNB_CONFIG_AVAILABLE  # otomatis off kalau tidak tersedia\n","USE_LORA = PEFT_AVAILABLE  # otomatis off kalau peft tidak tersedia\n","\n","print(\"USE_4BIT:\", USE_4BIT)\n","print(\"USE_LORA:\", USE_LORA)\n"]},{"cell_type":"markdown","id":"0804cf04","metadata":{"id":"0804cf04"},"source":["## 1. Load dataset & model\n","\n","**TODO:** pastikan nama model Phi-2 benar sesuai HuggingFace Hub yang kamu pakai.\n","\n","Dataset XSum fields umumnya:\n","- `document`\n","- `summary`"]},{"cell_type":"code","execution_count":5,"id":"56f33ac7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"56f33ac7","outputId":"97572c57-1037-4534-9b66-49f9214f032a","executionInfo":{"status":"ok","timestamp":1768120444668,"user_tz":-420,"elapsed":26456,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['document', 'summary', 'id'],\n","        num_rows: 204045\n","    })\n","    validation: Dataset({\n","        features: ['document', 'summary', 'id'],\n","        num_rows: 11332\n","    })\n","    test: Dataset({\n","        features: ['document', 'summary', 'id'],\n","        num_rows: 11334\n","    })\n","})\n"]}],"source":["from datasets import load_dataset\n","import evaluate\n","\n","DATASET_REPO = \"EdinburghNLP/xsum\"\n","REV = \"refs%2Fconvert%2Fparquet\"  # URL-encoded\n","\n","dataset = load_dataset(\n","    \"parquet\",\n","    data_files={\n","        \"train\":      f\"hf://datasets/{DATASET_REPO}@{REV}/default/train/*.parquet\",\n","        \"validation\": f\"hf://datasets/{DATASET_REPO}@{REV}/default/validation/*.parquet\",\n","        \"test\":       f\"hf://datasets/{DATASET_REPO}@{REV}/default/test/*.parquet\",\n","    },\n",")\n","\n","print(dataset)\n","metric = evaluate.load(\"rouge\")\n"]},{"cell_type":"code","execution_count":6,"id":"4yLaf7OXHdY3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4yLaf7OXHdY3","outputId":"9aeb8141-ebbf-4d1a-d595-280f315ae6d9","executionInfo":{"status":"ok","timestamp":1768120444678,"user_tz":-420,"elapsed":13,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['document', 'summary', 'id']\n","{'document': 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.', 'summary': 'Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.', 'id': '35232142'}\n"]}],"source":["print(dataset[\"train\"].column_names)\n","print(dataset[\"train\"][0])\n"]},{"cell_type":"markdown","id":"bc5a1506","metadata":{"id":"bc5a1506"},"source":["## 2. Load tokenizer & model\n","\n","Untuk model causal LM:\n","- kita membuat prompt \"Summarize: {document}\\nSummary:\" lalu targetnya `summary`\n","- saat training, label biasanya sama dengan input_ids (shift internal oleh model)\n","\n","**TODO:** cek `tokenizer.pad_token` (beberapa LLM tidak punya pad token)."]},{"cell_type":"code","execution_count":7,"id":"a2e742e4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a2e742e4","executionInfo":{"status":"ok","timestamp":1768120444692,"user_tz":-420,"elapsed":13,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"952fbff6-eec3-4fc7-f419-9b475ef2e3e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["MODEL_NAME: microsoft/phi-2\n","USE_4BIT_LORA: True\n"]}],"source":["# =========================\n","# Configuration (edit here)\n","# =========================\n","\n","# Model (Phi-2)\n","MODEL_NAME = \"microsoft/phi-2\"\n","\n","# Sequence length for training examples (prompt + summary)\n","# If you get OOM, reduce to 384 or 256.\n","MAX_LENGTH = 512\n","\n","# Optional: limit raw document chars before tokenization (helps keep prompts smaller)\n","MAX_DOC_CHARS = 4000\n","\n","# Training hyperparameters (safe defaults for Colab GPU)\n","BATCH_SIZE = 1\n","GRAD_ACCUM = 8          # effective batch = BATCH_SIZE * GRAD_ACCUM\n","EPOCHS = 1\n","LR = 2e-4\n","\n","# LoRA + 4-bit (optional)\n","# Auto-enable only if CUDA + BitsAndBytesConfig + PEFT are available.\n","USE_4BIT_LORA = bool(device == \"cuda\" and BNB_CONFIG_AVAILABLE and PEFT_AVAILABLE)\n","\n","print(\"MODEL_NAME:\", MODEL_NAME)\n","print(\"USE_4BIT_LORA:\", USE_4BIT_LORA)\n"]},{"cell_type":"code","execution_count":8,"id":"99e7ddf4","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["3b8a01ba0abf45359f3da1c1db51ab65","d2dda745389b46b1974322b70e8c02de","4476353ce2b544d6a23523fd547a7d94","68c6dfb12465441b93fd4ff5215af1f5","fa5a7c08b7464e7c92a45fd7ad46c1b9","08cfae33842f4cf39469da166871e977","ffebb1741a6e4c45a4bc388e233bded9","8ab9d9187a09433cbd33af409dfdf2cf","6252f2d642d14ebaa207fbd948619899","c07dae08b8eb434a84120ec991514c5a","a6214cd2a4434926aaa9e8824fa58030"]},"id":"99e7ddf4","outputId":"b7b80f66-68e7-41c6-fea9-e8880c45a01b","executionInfo":{"status":"ok","timestamp":1768120563803,"user_tz":-420,"elapsed":119109,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b8a01ba0abf45359f3da1c1db51ab65"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["trainable params: 18,350,080 || all params: 2,798,033,920 || trainable%: 0.6558\n"]}],"source":["# Load tokenizer & model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n","\n","# Phi-2 tokenizer doesn't always define pad_token by default.\n","tokenizer.padding_side = \"right\"\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","if USE_4BIT_LORA:\n","    # 4-bit quantization + LoRA (requires bitsandbytes + peft)\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.float16,\n","        bnb_4bit_use_double_quant=True,\n","    )\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_NAME,\n","        quantization_config=bnb_config,\n","        device_map=\"auto\",\n","    )\n","\n","    model = prepare_model_for_kbit_training(model)\n","\n","    lora_config = LoraConfig(\n","        r=16,\n","        lora_alpha=32,\n","        lora_dropout=0.05,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","    )\n","    model = get_peft_model(model, lora_config)\n","    model.print_trainable_parameters()\n","else:\n","    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n","\n","# Make sure model knows the pad token (important for generation & padding)\n","model.config.pad_token_id = tokenizer.pad_token_id\n"]},{"cell_type":"markdown","id":"9e396369","metadata":{"id":"9e396369"},"source":["## 3. Preprocessing\n","\n"]},{"cell_type":"code","execution_count":9,"id":"08aa2936","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369,"referenced_widgets":["0e621421c0404e48a32517ff6e35b2d9","3b7e12bf13c34d238b91708b09cb46ee","311e1974812d4d9d8903a1d165e79036","732461eb71f94616850a2d83e0cfc51c","7fdca137e076407988e70fa0f14a061c","af1cb2763c3b425d8585a5e5ad2b5a13","857a13773b254ff699fada273c40ef36","8ab319c90d4e44adb4e451ec557a8b75","84544975b7b04896821970af3718968a","aee1b093e88d47df85fea12b251ce3a7","78a222938d674e37bf50e0441cd3e258","3ba96f8d5dfb42528f29c29fb684edeb","bf5a5ffb73684de5bf0203259bc5e47d","944957b733a94e54898282b233e8fce8","c2ec61d9fa7c43acb2e401ca7eecfb93","f14b20287ee745b48b6aa73a0258b2eb","cc01652ee8b04d6ea1ad85de6667f379","46ceb8a794fc44e9b447d04c5cb51174","23954970a2f7460caad83e524c834e37","c3a2b0554ffb4c198686bc6325f3eea4","1202b82596654bd190fed801dd8d178f","1d640b5701cb43608380491c36092877","53b83e84e80746a3946d19b05295d558","3e49f71aa0b947bc8d19c2b86bbd9b0f","4e096e2e835048dfabbfdf829aa9608b","04fe94c3ce494cde9090388dc51607e5","a4973b72fe49471fb8525e8d23a108d2","79aeec3a0d614978956526c79fdd09d6","d11951b4fb784587bf85e9a42c710cf5","e04aed1ca51b4dd5b9ab2530fc889069","236c15c7e2f6441d926db5c54f14335c","d4dcee2f498e4033a227f1e153ed66b8","50bc36baa5f944c892da88e1d078be5d"]},"id":"08aa2936","outputId":"178f391d-ecc2-4452-f6e8-1be293ba353c","executionInfo":{"status":"ok","timestamp":1768120664149,"user_tz":-420,"elapsed":100321,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/204045 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e621421c0404e48a32517ff6e35b2d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/11332 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ba96f8d5dfb42528f29c29fb684edeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/11334 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53b83e84e80746a3946d19b05295d558"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'attention_mask'],\n","        num_rows: 204045\n","    })\n","    validation: Dataset({\n","        features: ['input_ids', 'attention_mask'],\n","        num_rows: 11332\n","    })\n","    test: Dataset({\n","        features: ['input_ids', 'attention_mask'],\n","        num_rows: 11334\n","    })\n","})\n"]}],"source":["# Preprocessing / Tokenization\n","# We will train Phi-2 as a causal LM on (prompt + summary).\n","# IMPORTANT: do NOT create 'labels' here. Let the data collator handle labels + padding\n","# to avoid tensor shape errors during batching.\n","\n","def build_prompt(doc: str) -> str:\n","    doc = doc[:MAX_DOC_CHARS]\n","    return (\n","        \"Summarize the following article in 1-2 sentences.\\n\\n\"\n","        f\"Article:\\n{doc}\\n\\n\"\n","        \"Summary:\"\n","    )\n","\n","def preprocess_batch(examples):\n","    docs = examples[\"document\"]\n","    sums = examples[\"summary\"]\n","\n","    texts = []\n","    for d, s in zip(docs, sums):\n","        prompt = build_prompt(d)\n","        full = prompt + \" \" + s + tokenizer.eos_token\n","        texts.append(full)\n","\n","    tokenized = tokenizer(\n","        texts,\n","        truncation=True,\n","        max_length=MAX_LENGTH,\n","        padding=False,  # padding will be done dynamically by the data collator\n","    )\n","    return tokenized\n","\n","tokenized = dataset.map(preprocess_batch, batched=True, remove_columns=dataset[\"train\"].column_names)\n","print(tokenized)\n"]},{"cell_type":"markdown","id":"5cdb6305","metadata":{"id":"5cdb6305"},"source":["## 4. Trainer\n","\n","**TODO:** atur output_dir & logging."]},{"cell_type":"code","execution_count":10,"id":"2e8db095","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2e8db095","outputId":"82db7f2f-8ed5-414e-dba4-f4e3824fabac","executionInfo":{"status":"ok","timestamp":1768120664188,"user_tz":-420,"elapsed":31,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2854578672.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]}],"source":["data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","# NOTE: Some older transformers versions don't support evaluation_strategy.\n","# We build TrainingArguments in a version-compatible way.\n","import inspect\n","from transformers import TrainingArguments\n","\n","use_fp16 = bool(torch.cuda.is_available())\n","\n","base_kwargs = dict(\n","    output_dir=str(OUTPUTS_DIR),\n","    learning_rate=LR,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    gradient_accumulation_steps=GRAD_ACCUM,\n","    num_train_epochs=EPOCHS,\n","    logging_steps=50,\n","    save_steps=500,\n","    save_total_limit=2,\n","    fp16=use_fp16,\n","    report_to=\"none\",\n",")\n","\n","sig = inspect.signature(TrainingArguments.__init__)\n","params = sig.parameters\n","\n","# Newer versions\n","if \"evaluation_strategy\" in params:\n","    base_kwargs.update(dict(\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","    ))\n","else:\n","    # Older versions (fallback)\n","    if \"evaluate_during_training\" in params:\n","        base_kwargs[\"evaluate_during_training\"] = True\n","    if \"eval_steps\" in params:\n","        base_kwargs[\"eval_steps\"] = 500\n","\n","# Keep only supported args for this transformers version\n","filtered_kwargs = {k: v for k, v in base_kwargs.items() if k in params}\n","\n","args = TrainingArguments(**filtered_kwargs)\n","\n","trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=tokenized[\"train\"],\n","    eval_dataset=tokenized[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n"]},{"cell_type":"markdown","id":"baa80a3f","metadata":{"id":"baa80a3f"},"source":["## 5. Training\n","\n","**TODO:** jalankan training."]},{"cell_type":"code","execution_count":12,"id":"407a3706","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":466},"id":"407a3706","outputId":"5348e11f-d703-4018-8d92-0080c2b05936","executionInfo":{"status":"error","timestamp":1768120964719,"user_tz":-420,"elapsed":33240,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5' max='25506' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [    5/25506 00:20 < 47:45:17, 0.15 it/s, Epoch 0.00/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1401853779.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mDO_TRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2846\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2847\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2848\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2849\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Train / Evaluate / Save artifacts to Google Drive\n","# If you already trained and only want to re-run evaluation, set DO_TRAIN = False.\n","import json\n","from datetime import datetime\n","\n","DO_TRAIN = True\n","\n","if DO_TRAIN:\n","    train_result = trainer.train()\n","    print(train_result)\n","\n","# Save model + tokenizer\n","run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","save_dir = MODELS_DIR / f\"phi2_xsum_{run_id}\"\n","save_dir.mkdir(parents=True, exist_ok=True)\n","\n","trainer.save_model(str(save_dir))\n","tokenizer.save_pretrained(str(save_dir))\n","print(\"Saved model to:\", save_dir)\n","\n","# Evaluate (loss on validation by default)\n","eval_metrics = trainer.evaluate()\n","print(\"Eval metrics:\", eval_metrics)\n","\n","# Save metrics\n","metrics_path = REPORTS_DIR / \"metrics.json\"\n","with open(metrics_path, \"w\") as f:\n","    json.dump(eval_metrics, f, indent=2)\n","print(\"Saved metrics to:\", metrics_path)\n"]},{"cell_type":"markdown","id":"46ac57b5","metadata":{"id":"46ac57b5"},"source":["## 6. Evaluasi ROUGE (setelah training)\n","\n","Evaluasi summarization biasanya:\n","- generate summary dari prompt\n","- bandingkan dengan reference summary (ROUGE)\n","\n","**TODO:** jalankan cell ini setelah training (dan mungkin pakai subset agar cepat)."]},{"cell_type":"code","execution_count":null,"id":"fb0f1015","metadata":{"id":"fb0f1015","executionInfo":{"status":"aborted","timestamp":1768120930498,"user_tz":-420,"elapsed":575413,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["# (Optional) Evaluasi ROUGE cepat pada subset validation\n","# Uncomment dan jalankan setelah training kalau kamu ingin ROUGE.\n","#\n","# def generate_summary(doc: str, max_new_tokens: int = 64):\n","#     prompt = build_prompt(doc)\n","#     inputs = tokenizer(\n","#         prompt,\n","#         return_tensors=\"pt\",\n","#         truncation=True,\n","#         max_length=MAX_LENGTH,\n","#         padding=True,\n","#     ).to(model.device)\n","#     with torch.no_grad():\n","#         out = model.generate(\n","#             **inputs,\n","#             max_new_tokens=max_new_tokens,\n","#             do_sample=False,\n","#             num_beams=4,\n","#             pad_token_id=tokenizer.pad_token_id,\n","#         )\n","#     text = tokenizer.decode(out[0], skip_special_tokens=True)\n","#     if \"Summary:\" in text:\n","#         text = text.split(\"Summary:\", 1)[-1].strip()\n","#     return text\n","#\n","# n = 200\n","# preds, refs = [], []\n","# for ex in dataset[\"validation\"].select(range(n)):\n","#     preds.append(generate_summary(ex[\"document\"]))\n","#     refs.append(ex[\"summary\"])\n","#\n","# rouge = metric.compute(predictions=preds, references=refs)\n","# print(rouge)\n"]},{"cell_type":"markdown","id":"070ffde6","metadata":{"id":"070ffde6"},"source":["## 7. Analisis\n","\n","**TODO:** isi `reports/` dengan:\n","- ROUGE score\n","- contoh hasil summary bagus vs buruk\n","- diskusi abstractive vs extractive\n","- kendala truncation & panjang dokumen"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3b8a01ba0abf45359f3da1c1db51ab65":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d2dda745389b46b1974322b70e8c02de","IPY_MODEL_4476353ce2b544d6a23523fd547a7d94","IPY_MODEL_68c6dfb12465441b93fd4ff5215af1f5"],"layout":"IPY_MODEL_fa5a7c08b7464e7c92a45fd7ad46c1b9"}},"d2dda745389b46b1974322b70e8c02de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08cfae33842f4cf39469da166871e977","placeholder":"​","style":"IPY_MODEL_ffebb1741a6e4c45a4bc388e233bded9","value":"Loading checkpoint shards: 100%"}},"4476353ce2b544d6a23523fd547a7d94":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ab9d9187a09433cbd33af409dfdf2cf","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6252f2d642d14ebaa207fbd948619899","value":2}},"68c6dfb12465441b93fd4ff5215af1f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c07dae08b8eb434a84120ec991514c5a","placeholder":"​","style":"IPY_MODEL_a6214cd2a4434926aaa9e8824fa58030","value":" 2/2 [01:38&lt;00:00, 42.36s/it]"}},"fa5a7c08b7464e7c92a45fd7ad46c1b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08cfae33842f4cf39469da166871e977":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffebb1741a6e4c45a4bc388e233bded9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ab9d9187a09433cbd33af409dfdf2cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6252f2d642d14ebaa207fbd948619899":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c07dae08b8eb434a84120ec991514c5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6214cd2a4434926aaa9e8824fa58030":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e621421c0404e48a32517ff6e35b2d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b7e12bf13c34d238b91708b09cb46ee","IPY_MODEL_311e1974812d4d9d8903a1d165e79036","IPY_MODEL_732461eb71f94616850a2d83e0cfc51c"],"layout":"IPY_MODEL_7fdca137e076407988e70fa0f14a061c"}},"3b7e12bf13c34d238b91708b09cb46ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af1cb2763c3b425d8585a5e5ad2b5a13","placeholder":"​","style":"IPY_MODEL_857a13773b254ff699fada273c40ef36","value":"Map: 100%"}},"311e1974812d4d9d8903a1d165e79036":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ab319c90d4e44adb4e451ec557a8b75","max":204045,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84544975b7b04896821970af3718968a","value":204045}},"732461eb71f94616850a2d83e0cfc51c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aee1b093e88d47df85fea12b251ce3a7","placeholder":"​","style":"IPY_MODEL_78a222938d674e37bf50e0441cd3e258","value":" 204045/204045 [01:30&lt;00:00, 2428.83 examples/s]"}},"7fdca137e076407988e70fa0f14a061c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af1cb2763c3b425d8585a5e5ad2b5a13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"857a13773b254ff699fada273c40ef36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ab319c90d4e44adb4e451ec557a8b75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84544975b7b04896821970af3718968a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aee1b093e88d47df85fea12b251ce3a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78a222938d674e37bf50e0441cd3e258":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ba96f8d5dfb42528f29c29fb684edeb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf5a5ffb73684de5bf0203259bc5e47d","IPY_MODEL_944957b733a94e54898282b233e8fce8","IPY_MODEL_c2ec61d9fa7c43acb2e401ca7eecfb93"],"layout":"IPY_MODEL_f14b20287ee745b48b6aa73a0258b2eb"}},"bf5a5ffb73684de5bf0203259bc5e47d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc01652ee8b04d6ea1ad85de6667f379","placeholder":"​","style":"IPY_MODEL_46ceb8a794fc44e9b447d04c5cb51174","value":"Map: 100%"}},"944957b733a94e54898282b233e8fce8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_23954970a2f7460caad83e524c834e37","max":11332,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3a2b0554ffb4c198686bc6325f3eea4","value":11332}},"c2ec61d9fa7c43acb2e401ca7eecfb93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1202b82596654bd190fed801dd8d178f","placeholder":"​","style":"IPY_MODEL_1d640b5701cb43608380491c36092877","value":" 11332/11332 [00:04&lt;00:00, 2420.16 examples/s]"}},"f14b20287ee745b48b6aa73a0258b2eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc01652ee8b04d6ea1ad85de6667f379":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46ceb8a794fc44e9b447d04c5cb51174":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23954970a2f7460caad83e524c834e37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3a2b0554ffb4c198686bc6325f3eea4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1202b82596654bd190fed801dd8d178f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d640b5701cb43608380491c36092877":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53b83e84e80746a3946d19b05295d558":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e49f71aa0b947bc8d19c2b86bbd9b0f","IPY_MODEL_4e096e2e835048dfabbfdf829aa9608b","IPY_MODEL_04fe94c3ce494cde9090388dc51607e5"],"layout":"IPY_MODEL_a4973b72fe49471fb8525e8d23a108d2"}},"3e49f71aa0b947bc8d19c2b86bbd9b0f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79aeec3a0d614978956526c79fdd09d6","placeholder":"​","style":"IPY_MODEL_d11951b4fb784587bf85e9a42c710cf5","value":"Map: 100%"}},"4e096e2e835048dfabbfdf829aa9608b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e04aed1ca51b4dd5b9ab2530fc889069","max":11334,"min":0,"orientation":"horizontal","style":"IPY_MODEL_236c15c7e2f6441d926db5c54f14335c","value":11334}},"04fe94c3ce494cde9090388dc51607e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4dcee2f498e4033a227f1e153ed66b8","placeholder":"​","style":"IPY_MODEL_50bc36baa5f944c892da88e1d078be5d","value":" 11334/11334 [00:04&lt;00:00, 2471.44 examples/s]"}},"a4973b72fe49471fb8525e8d23a108d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79aeec3a0d614978956526c79fdd09d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d11951b4fb784587bf85e9a42c710cf5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e04aed1ca51b4dd5b9ab2530fc889069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"236c15c7e2f6441d926db5c54f14335c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d4dcee2f498e4033a227f1e153ed66b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50bc36baa5f944c892da88e1d078be5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}